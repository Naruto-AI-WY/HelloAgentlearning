# 第三章《大语言模型基础》学习笔记 



## 目录

- [第一部分：学习笔记（零基础友好）](#第一部分学习笔记零基础友好)
  - [1. 语言模型到底在做什么](#1-语言模型到底在做什么)
  - [2. N-gram：统计语言模型](#2-n-gram统计语言模型)
  - [3. 词向量与神经网络语言模型：RNN/LSTM](#3-词向量与神经网络语言模型rnnlstm)
  - [4. Transformer：现代大模型的基础](#4-transformer现代大模型的基础)
  - [5. Decoder-Only：为什么 GPT 类模型这么流行](#5-decoder-only为什么-gpt-类模型这么流行)
  - [6. 与大模型交互：提示工程与采样参数](#6-与大模型交互提示工程与采样参数)
  - [7. Tokenization 与 BPE：为什么要子词](#7-tokenization-与-bpe为什么要子词)
  - [8. 本地调用开源模型：最小流程](#8-本地调用开源模型最小流程)
  - [9. 模型选择：开源 vs 闭源](#9-模型选择开源-vs-闭源)
  - [10. 缩放法则与幻觉：为什么越大越强也会胡说](#10-缩放法则与幻觉为什么越大越强也会胡说)
  - [11. 练习题解答](#11-练习题解答)
- [第二部分：作业模板（把你的实验结果填进去）](#第二部分作业模板把你的实验结果填进去)

---

# 第一部分：学习笔记（零基础友好）

## 1. 语言模型到底在做什么？

**一句话**：语言模型在做“自动续写”。  
给定前文，它要做的就是：**预测下一个词（或 token）最可能是什么**。

把一句话看成词序列 \(w_1,w_2,...,w_m\)，整句概率可以拆成条件概率连乘（链式法则）：

\[
P(w_1,w_2,...,w_m)=\prod_{i=1}^{m} P(w_i|w_1,...,w_{i-1})
\]

难点在于：真实语料中“长前缀”的统计非常稀疏，直接估计几乎不可能。

---

## 2. N-gram：统计语言模型

### 2.1 核心：马尔可夫假设
N-gram 用一个关键假设把难题变简单：

> **马尔可夫假设**：预测当前词时，不必看全部历史，只看最近 \(n-1\) 个词就够了。

- Bigram（N=2）：只看前 1 个词  
- Trigram（N=3）：只看前 2 个词

### 2.2 Bigram 怎么算概率？
Bigram 常用“数数”的方式（最大似然估计）：

\[
P(w_i|w_{i-1})=\frac{Count(w_{i-1},w_i)}{Count(w_{i-1})}
\]

---

## 3. 词向量与神经网络语言模型：RNN/LSTM

### 3.1 为什么要词向量（Embedding）？
N-gram 把词当“离散编号”，不理解语义相近关系。  
Embedding 把词变成向量，使得“语义相近的词”在向量空间里更近（可以用余弦相似度衡量）。

这带来的好处是：即使模型没见过某个组合，也可能根据“相似词”的经验做出合理泛化。

### 3.2 RNN：把历史压进隐藏状态
RNN 的核心直觉：  
> 用隐藏状态 \(h_t\) 作为“历史摘要”，每一步更新一次。

这样上下文长度不再固定为 N-1（不像 N-gram），理论上可以利用更长历史。

但 RNN 有明显工程/训练痛点：
- 必须串行（一步依赖上一步） → 训练慢、难并行
- 梯度消失/爆炸 → 长距离依赖学不稳

### 3.3 LSTM：门控记忆（更能记住长期信息）
LSTM 在 RNN 上加入“门”（遗忘门/输入门/输出门）与细胞状态，让重要信息更容易长距离传播，缓解梯度消失问题。

---

## 4. Transformer：现代大模型的基础

Transformer 把“序列建模”的核心从“递推（RNN）”换成了“注意力（Attention）”。

### 4.1 Self-Attention 的核心思想
**一句话**：每个 token 都能“看见”序列中的其他 token，并学习“我该重点关注谁”。

常用理解方式是 Q/K/V：
- **Q（Query）**：我现在要找什么信息？
- **K（Key）**：我能提供什么索引特征？
- **V（Value）**：我真正携带的内容信息是什么？

注意力权重（softmax 后）可以理解成“关注程度”，再对 V 做加权求和得到新的表示。

### 4.2 为什么 Transformer 能并行，而 RNN 必须串行？
- **RNN 串行**：第 t 步依赖第 t-1 步的隐藏状态，必须一步步算。
- **Transformer 并行**：同一层中所有位置的 Q/K/V 可同时算，注意力是矩阵计算，天然适合 GPU 并行。

### 4.3 位置编码（Positional Encoding）是干什么的？
注意力机制本身并不知道“顺序”（只知道相似度）。  
位置编码把“第几个 token”的位置信息注入表示中，让模型知道先后顺序。

### 4.4 多头注意力 + FFN + Add&Norm（你至少要认识这些模块）
- **Multi-Head Attention**：从多个“子空间视角”同时做注意力，信息表达更丰富。
- **FFN（前馈网络）**：对每个位置做非线性变换，增强表达能力。
- **残差连接 + LayerNorm（Add & Norm）**：让深层网络训练更稳定、收敛更容易。

---

## 5. Decoder-Only：为什么 GPT 类模型这么流行

### 5.1 Encoder-Decoder vs Decoder-Only
- **Encoder-Decoder**：Encoder “理解输入”，Decoder “生成输出”，常见于翻译、摘要等“输入→输出映射”任务。
- **Decoder-Only**：只保留 Decoder，用“因果 Mask”保证只能看见前文，统一做 next-token prediction（预测下一个 token）。

### 5.2 为什么主流大模型多采用 Decoder-Only？
常见原因：
- 训练目标统一（next-token），便于用海量无标注文本预训练
- 结构更简单，更容易扩展到超大规模
- 天然适配生成任务（对话/写作/代码）

---

## 6. 与大模型交互：提示工程与采样参数

### 6.1 提示工程（Prompt Engineering）
Prompt 就是你对模型的“说明书”。写得越清晰、约束越明确，输出越稳定。

常见策略：
- **Zero-shot**：直接给任务描述
- **Few-shot**：给 2~3 个示例“现场教学”
- **Chain-of-Thought（CoT）**：要求模型分步骤推理（适合多步推理/复杂抽取）

### 6.2 采样参数：Temperature / Top-k / Top-p
这些参数控制“输出更确定还是更发散”：
- **Temperature**：越低越保守，越高越随机
- **Top-k**：只从概率最高的 k 个候选里采样
- **Top-p**：从累计概率达到 p 的最小候选集合里采样（更自适应）

---

## 7. Tokenization 与 BPE：为什么要子词？

模型不能直接吃“自然语言字符串”，通常要把文本切成 token 再映射到 token id。

### 7.1 为什么不用“字符”？
- 序列会很长 → 训练/推理更慢
- 学到“词级语义”更费劲

### 7.2 为什么不用“单词”？
- 词表会巨大 → 成本高
- OOV（未登录词）严重：新词、专有名词、拼写变体会大量出现

### 7.3 BPE 解决什么？
**BPE（Byte Pair Encoding）**用“子词（subword）”折中：
- 常见词可以合并成整体（更短更快）
- 生僻词可以拆成多个子词（不至于 OOV）
- 词表大小可控、覆盖能力强

---

## 8. 本地调用开源模型：最小流程

你至少要能说清楚这条链路：

1) 选模型（CausalLM/Seq2Seq 等）  
2) 加载 tokenizer  
3) 加载模型权重  
4) 准备输入（对话模板/普通文本）  
5) tokenize → 得到 token ids  
6) `generate()` 生成  
7) decode → 得到可读文本

---

## 9. 模型选择：开源 vs 闭源

| 维度 | 闭源（API） | 开源（自部署/私有化） |
|---|---|---|
| 性能上限 | 往往更强、更稳 | 取决于模型与调优 |
| 成本结构 | 按量计费（token） | 硬件+运维为主 |
| 可控性/可定制 | 平台限制多 | 可微调、可改系统 |
| 隐私合规 | 可能出网（可选合规方案） | 可不出网，更易合规 |
| 运维复杂度 | 低 | 较高 |

---

## 10. 缩放法则与幻觉：为什么越大越强也会胡说

### 10.1 缩放法则（Scaling Laws）
经验上，模型性能与参数量/数据量/算力之间存在可预测趋势：规模上去，能力上去（直到遇到瓶颈）。

### 10.2 模型幻觉（Hallucination）
幻觉指：模型生成了**看起来很像真的、但与事实或原文不一致**的内容。  
常见原因：模型本质是在做“下一个 token 的概率预测”，并不自带事实核查机制。

### 10.3 缓解幻觉的常用方法
- **检索增强生成（RAG）**：先检索证据，再基于证据生成
- **多步推理与自检**：先草稿，再核查，再修订
- **外部工具调用**：搜索/计算/代码执行，提高可验证性
- **RLHF 等训练层面方法**：提升对齐与可靠性（工程上常与 RAG 配合）

---

## 11. 练习题解答

### 11.1 计算 `agent works` 在 Bigram 下的概率

迷你语料库：
- `datawhale agent learns`
- `datawhale agent works`

合并 tokens：`datawhale, agent, learns, datawhale, agent, works`（共 6 个 token）

1) \(P(agent)=Count(agent)/总token=2/6=1/3\)  
2) \(P(works|agent)=Count(agent,works)/Count(agent)=1/2\)  
3) \(P(agent\ works)=P(agent)\cdot P(works|agent)=\frac{1}{3}\cdot\frac{1}{2}=\frac{1}{6}\approx 0.167\)

---

### 11.2 马尔可夫假设 + N-gram 局限

**马尔可夫假设含义**：预测当前词时，只看最近 \(n-1\) 个词，不看更久远的历史。

**N-gram 根本局限（核心点）**
1) 数据稀疏：没见过的组合概率趋近 0  
2) 不理解语义相似：无法“举一反三”  
3) 长距离依赖弱：窗口外的信息看不见  
4) N 增大参数爆炸：对语料规模要求极高

---

### 11.3 RNN/LSTM 与 Transformer 如何克服 N-gram？

- **RNN/LSTM**：用隐藏状态携带历史摘要，上下文长度不固定；LSTM 用门控增强长期记忆能力。  
  - 优势：对顺序天然敏感；实现思路直观  
  - 不足：串行、难并行；长序列训练更难

- **Transformer**：自注意力让每个位置直接聚合全局信息，并行计算效率高。  
  - 优势：并行、长依赖强、可扩展性好  
  - 不足：长序列注意力计算成本更高（工程上常用稀疏注意力/滑窗等优化）

---

### 11.4 Transformer 问答

**(1) Self-Attention 核心思想**：每个 token 学会“该关注谁”，用加权汇总的方式融合上下文信息（Q/K/V）。

**(2) 并行 vs 串行**：RNN 的递推依赖导致串行；Transformer 的矩阵化注意力允许并行。  
位置编码提供顺序信息，否则注意力无法区分先后。

**(3) Decoder-Only vs Encoder-Decoder**：Encoder-Decoder 有编码器理解输入、解码器生成输出；Decoder-Only 仅靠因果 Mask 做 next-token 生成。  
主流大模型偏向 Decoder-Only：训练目标统一、结构更简单、生成任务适配性强。

---

### 11.5 分词与 BPE

- 字符级：序列长、计算慢、语义学习效率低  
- 单词级：词表巨大、OOV 严重  
- BPE：用子词折中，实现“词表可控 + 覆盖新词 + 序列长度适中”

---

# 第二部分：作业模板

> 使用方式：把本部分复制出来当作“作业提交文档”，然后把你的真实运行结果粘贴到空白处即可。

---

## 0. 基本信息

- 课程/章节：第三章 大语言模型基础  
- 姓名：  
- 学号：  
- 日期（YYYY-MM-DD）：  
- 运行环境：
  - 操作系统：  
  - Python 版本：  
  - GPU/CPU：  
  - 关键依赖版本（transformers/torch 等）：  

---

## 1. 理论题（必做）

### 1.1 Bigram 概率计算：`agent works`

**迷你语料库：**
- `datawhale agent learns`
- `datawhale agent works`

**要求：**计算句子 `agent works` 在 Bigram 模型下的概率。

**计算过程（填写）：**
1) 统计语料总 token 数：`_____`  
2) Count(agent)：`_____`  
3) \(P(agent)=Count(agent)/总token=\) `_____`  
4) Count(agent, works)：`_____`  
5) Count(agent)：`_____`  
6) \(P(works|agent)=Count(agent,works)/Count(agent)=\) `_____`  
7) \(P(agent\ works)=P(agent)\cdot P(works|agent)=\) `_____`

**最终答案：**
- \(P(agent\ works)=\) `_____`

---

### 1.2 马尔可夫假设与 N-gram 局限

**问题 1：马尔可夫假设含义（用自己的话解释）：**  
- `（填写）`

**问题 2：N-gram 的根本局限（至少 3 点）：**
1. `（填写）`
2. `（填写）`
3. `（填写）`
4. （可选）`（填写）`

---

### 1.3 RNN/LSTM 与 Transformer 如何克服 N-gram 局限？各自优势？

**RNN/LSTM：**
- 如何改进 N-gram：`（填写）`
- 优势：`（填写）`
- 不足（可选）：`（填写）`

**Transformer：**
- 如何改进 N-gram：`（填写）`
- 优势：`（填写）`
- 不足（可选）：`（填写）`

---

### 1.4 Transformer 关键概念题

#### 1.4.1 Self-Attention 的核心思想
- `（填写：用通俗语言解释 Q/K/V + 关注权重是什么）`

#### 1.4.2 为什么 Transformer 能并行而 RNN 必须串行？位置编码起什么作用？
- 并行 vs 串行原因：`（填写）`
- 位置编码作用：`（填写）`

#### 1.4.3 Decoder-Only vs Encoder-Decoder：区别与主流原因
- 架构区别：`（填写）`
- 为什么主流 LLM 多选 Decoder-Only：`（填写）`

---

### 1.5 分词与 BPE

**问题 1：为什么不能直接用“字符”作为输入单元？**  
- `（填写）`

**问题 2：为什么不能直接用“单词”作为输入单元？**  
- `（填写）`

**问题 3：BPE 解决了什么问题？**  
- `（填写）`

---

## 2. 实践题（必做：本地部署 + 调参观察）

> 推荐：本地部署一个轻量开源模型（例如 0.5B~1B 量级），并做采样参数对比实验。  
> 提醒：请以你实际使用的模型卡与官方文档为准（不同模型的 chat template、特殊参数可能不同）。

### 2.1 本地部署记录

**选择的模型：**
- 模型名称（HuggingFace ID）：`_____`
- 模型类型（CausalLM / Seq2Seq 等）：`_____`
- 是否量化（8bit/4bit/未量化）：`_____`

**安装与运行命令（粘贴）：**
- 安装依赖：
```bash
# 在此粘贴你的 pip/conda 安装命令
```

- 运行脚本（或 Notebook 关键代码）：
```python
# 在此粘贴最小可运行推理代码
```

**成功运行证明（至少一种）：**
- [ ] 终端日志截图  
- [ ] 输出结果截图  
- [ ] 粘贴一段模型生成文本（如下）

**模型输出示例（粘贴）：**
- Prompt：
```text
（填写你用于测试的固定提示词）
```

- Output：
```text
（粘贴模型生成的结果）
```

---

### 2.2 采样参数实验：Temperature / Top-k / Top-p

> 要求：固定同一个 Prompt，仅调整采样参数，至少对比 **3 组**设置。  
> 建议：每组跑 2 次，观察稳定性与随机性。

**固定 Prompt（所有组都用同一句）：**
```text
（填写固定 prompt，例如：请用 150 字介绍“智能客服系统”的优势与局限）
```

#### 实验组 A
- 参数：temperature=`__`，top_p=`__`，top_k=`__`（可选）
- 输出 1：
```text
（粘贴）
```
- 输出 2（可选）：
```text
（粘贴）
```
- 观察结论：
  - `（填写：更确定/更发散/更跑题/更重复等）`

#### 实验组 B
- 参数：temperature=`__`，top_p=`__`，top_k=`__`
- 输出 1：
```text
（粘贴）
```
- 输出 2（可选）：
```text
（粘贴）
```
- 观察结论：
  - `（填写）`

#### 实验组 C
- 参数：temperature=`__`，top_p=`__`，top_k=`__`
- 输出 1：
```text
（粘贴）
```
- 输出 2（可选）：
```text
（粘贴）
```
- 观察结论：
  - `（填写）`

**总体对比总结（必须写）：**
- 哪组最稳定：`（填写）`
- 哪组最有创意：`（填写）`
- 哪组最容易跑题/幻觉：`（填写）`
- 你会在什么任务里选哪组参数：`（填写）`

---

## 3. 提示策略对比（必做）

> 选择一个具体任务：  
> - [ ] 文本分类  
> - [ ] 信息抽取  
> - [ ] 代码生成  
> - [ ] 其他：_____

### 3.1 任务定义

- 任务名称：`_____`
- 输入样例（至少 3 条）：  
  1. `_____`  
  2. `_____`  
  3. `_____`  
- 期望输出格式（例如 JSON/标签/代码等）：  
  - `（填写）`

---

### 3.2 Zero-shot

**Prompt：**
```text
（粘贴 Zero-shot 提示词）
```

**输出结果（粘贴至少 3 条输入对应输出）：**
- 样例 1：
```text
（粘贴）
```
- 样例 2：
```text
（粘贴）
```
- 样例 3：
```text
（粘贴）
```

**效果评价：**
- 优点：`（填写）`
- 缺点：`（填写）`

---

### 3.3 Few-shot

**Prompt（包含 2~3 个示例）：**
```text
（粘贴 Few-shot 提示词）
```

**输出结果（粘贴）：**
- 样例 1：
```text
（粘贴）
```
- 样例 2：
```text
（粘贴）
```
- 样例 3：
```text
（粘贴）
```

**效果评价：**
- 优点：`（填写）`
- 缺点：`（填写）`

---

### 3.4 Chain-of-Thought（CoT）

> 若课程要求不展示推理过程，可以写：  
> **“请先逐步思考，但最终只输出 JSON/标签/代码。”**

**Prompt：**
```text
（粘贴 CoT 提示词）
```

**输出结果（粘贴）：**
- 样例 1：
```text
（粘贴）
```
- 样例 2：
```text
（粘贴）
```
- 样例 3：
```text
（粘贴）
```

**效果评价：**
- 优点：`（填写）`
- 缺点：`（填写）`

---

### 3.5 三种策略总体对比结论（必须写）

- 最推荐策略：`Zero-shot / Few-shot / CoT`（选一个或组合）
- 推荐理由（结合准确率、格式稳定性、成本、时延）：  
  - `（填写）`

---

## 4. 模型选型：闭源 vs 开源（必做）

### 4.1 对比表（填写）

| 维度 | 闭源模型（API） | 开源模型（自部署） |
|---|---|---|
| 性能上限 |  |  |
| 成本结构 |  |  |
| 可控性/可定制 |  |  |
| 隐私与合规 |  |  |
| 运维复杂度 |  |  |
| 迭代更新 |  |  |

### 4.2 企业级客服智能体：你会选哪类模型？为什么？

**选择：**
- [ ] 闭源（API）
- [ ] 开源（自部署）
- [ ] 混合方案（推荐写清楚：哪些用开源、哪些用闭源）

**考虑因素（至少 5 点）：**
1. `（填写）`
2. `（填写）`
3. `（填写）`
4. `（填写）`
5. `（填写）`

---

## 5. 幻觉与缓解（必做）

### 5.1 选择一种方法：说明原理与适用场景
- 选择方法（RAG / 多步推理 / 工具调用 / 其他）：`_____`

**工作原理（用 5~8 句话讲清楚）：**
- `（填写）`

**适用场景（至少 3 个）：**
1. `（填写）`
2. `（填写）`
3. `（填写）`

---

### 5.2 前沿调研：还有哪些缓解幻觉方法？优势是什么？

> 建议至少列 2 篇论文/工作（写清楚方法名+核心思想+改进点）。

1) 方法/论文：`_____`  
- 核心思想：`（填写）`  
- 改进与优势：`（填写）`

2) 方法/论文：`_____`  
- 核心思想：`（填写）`  
- 改进与优势：`（填写）`

（可选）3) 方法/论文：`_____`

---

## 6. 论文辅助阅读智能体设计（必做）

### 6.1 基座模型选择
- 你选的基座模型：`_____`
- 选择理由（至少 5 点：长上下文/推理/成本/部署/合规等）：  
  1. `（填写）`
  2. `（填写）`
  3. `（填写）`
  4. `（填写）`
  5. `（填写）`

---

### 6.2 提示词设计（给出一份可用 Prompt）

> 要求：能引导模型做“摘要 + 问答 + 信息抽取 + 多论文对比”。

**Prompt 模板：**
```text
（在此粘贴你设计的提示词模板，建议包含：角色、任务、输出格式、引用要求、拒答规则）
```

---

### 6.3 长文超出上下文窗口：你如何解决？
- 方案（分块/索引/RAG/层级摘要等）：`（填写）`
- 你的步骤（用列表写清楚）：  
  1. `（填写）`
  2. `（填写）`
  3. `（填写）`
  4. `（填写）`

---

### 6.4 如何保证准确客观、忠于原文？
> 至少写 5 条系统设计（例如：强制引用、可追溯日志、多步核查、置信度、人工审核等）。

1. `（填写）`
2. `（填写）`
3. `（填写）`
4. `（填写）`
5. `（填写）`

---

## 7. 总结与反思（必写）

- 本章你认为最重要的 3 个知识点：  
  1. `（填写）`  
  2. `（填写）`  
  3. `（填写）`  

- 你在实践中遇到的 1~2 个问题，以及解决办法：  
  - 问题 1：`（填写）` → 解决：`（填写）`  
  - 问题 2（可选）：`（填写）` → 解决：`（填写）`  

- 下一步你想深入学习的内容：`（填写）`

---
